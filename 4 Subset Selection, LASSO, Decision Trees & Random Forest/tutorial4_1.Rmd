---
title: "Tutorial 4: Subset Selection & LASSO"
date: "2023-01-30"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
    code_download: TRUE
---

### Data

We'll use ebeer and telco data sets from before. We'll drop the ID column, and select customers that received a mailing only.

```{r, message=FALSE, warning=FALSE}
rm(list=ls())
library(tree)
library(dplyr)
library(janitor)
library(car)
library(pROC)
library(ranger)
library(glmnet)
library(readr)

options("scipen"=200, "digits"=3)
```

Ebeer

```{r, message=FALSE, warning=FALSE}
# set working directory using however you want to folder where data is stored.  I'll use 
ebeer <- read_csv("ebeer.csv")

# load ebeer, remove account number column
ebeer<-ebeer[-c(1)]
```

Training-Test Samples:

```{r}
# drop the ID column, select customers that received a mailing only
ebeer_test<-subset(ebeer, mailing ==1)

# create ebeer rollout data
ebeer_rollout<-subset(ebeer, mailing ==0)

# rename ebeer_test ebeer
ebeer<-ebeer_test
```

Telco

```{r, warning=FALSE, message=FALSE}
# load telco
telco <- read_csv("telco.csv")
telco <- strings2factors(telco)

# drop ID column, divide Total charges by 1000
telco<-subset(telco, select=-customerID)
telco$TotalCharges<-telco$TotalCharges/1000
```

Training-Test:

```{r}
# create 70% test and 30% holdout sample
set.seed(19103)
n <- nrow(telco)
sample <- sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.7, 0.3))
telco.test <- telco[sample, ]
telco.holdout <- telco[!sample, ]

#call test telco, and full data set telco.all
telco.all<-telco
telco<-telco.test
```

### Forward selection

The biggest model we consider is with all of the variables in churn plus all the two-way interactions with the variable `tenure`.

```{r}
full <- glm(Churn ~ . + tenure:(.), data=telco, family = "binomial")

summary(full)
```

We do forward selection. The algorithm uses AIC, but we could modify it by including `k=0` to consider deviance (with no penalty).

You can see it going through all the models. The `<none>` in the output below shows the best model (lowest AIC) at the current step.

```{r, cache=TRUE}
# intercept only
null <- glm(Churn ~ 1, data=telco, family = "binomial")
start_time <- Sys.time()
fwd.model <- step(null, direction = 'forward', scope=formula(full), keep = function(model, aic) list(model = model, aic = aic))
end_time <- Sys.time()
t <- end_time-start_time
```

Here is the table showing the variables added per step.

```{r}
fwd.model$anova
```

Note not all coefficients were added.

```{r, echo=FALSE}
cat("Coeficients included in Forward Selection:", length(fwd.model$coefficients), "\n")
cat("Total number of coeficients:",length(full$coefficients))
```

Now let's test the sequence of models with varying numbers of covariates using cross validation. We focus on OOS R2:

```{r, cache=TRUE}
M <- dim(fwd.model$keep)[2]

OOS=data.frame(R2=rep(NA,M), rank=rep(NA, M))


## pred must be probabilities (0<pred<1) for binomial
deviance <- function(y, pred, family=c("gaussian","binomial")){
    family <- match.arg(family)
    if(family=="gaussian"){
      return( sum( (y-pred)^2 ) )
    }else{
      if(is.factor(y)) y <- as.numeric(y)>1
      return( -2*sum( y*log(pred) + (1-y)*log(1-pred) ) )
    }
  }

## get null devaince too, and return R2
  R2 <- function(y, pred, family=c("gaussian","binomial")){
  fam <- match.arg(family)
  if(fam=="binomial"){
    if(is.factor(y)){ y <- as.numeric(y)>1 }
  }
  dev <- deviance(y, pred, family=fam)
  dev0 <- deviance(y, mean(y), family=fam)
  return(1-dev/dev0)
  }  

for(k in 1:M){

pred = predict(fwd.model$keep[["model",k]], newdata=telco.holdout, type = "response")

OOS$R2[k]<-R2(y = telco.holdout$Churn,pred=pred, family="binomial")
OOS$rank[k]<-fwd.model$keep[["model",k]]$rank
  
  
}
ax=c(1:max(OOS$rank))
par(mai=c(.9,.8,.2,.2))
plot(x=OOS$rank, y = OOS$R2, type="b", ylab=expression(paste("Out-of-Sample R"^"2")), xlab="# of model parameters estimated (rank)", xaxt="n")
axis(1, at=ax, labels=ax)

max.idx <- which.max(OOS$R2)

OOS$rank[max.idx]
abline(v=OOS$rank[max.idx], lty=3)

model <- fwd.model$keep[["model",max.idx]]

```

From the forward selection model path, the model with `r OOS$rank[max.idx]` coefficients has the best OOS R2.

We then "choose" this model, and re-estimate it using the entire dataset. The resulting coefficients are:

```{r}
model_full_data<-glm(model$formula, data = telco.all, family = binomial(link = "logit"))

summary(model_full_data)
```

### LASSO in R

We have to create our own matrix of dummy variables for factors. So we do that using the model.matrix() command.

```{r}
# all the factor variables
xfactors<- model.matrix(Churn ~ SeniorCitizen + Partner + Dependents + PhoneService + MultipleLines + InternetService + OnlineSecurity + OnlineBackup + DeviceProtection + TechSupport + StreamingTV + StreamingMovies + Contract + PaperlessBilling + PaymentMethod, data = telco)

# remove intercept
xfactors<-xfactors[,-1]

# all continuous variables
x<-as.matrix(data.frame(telco$tenure, telco$MonthlyCharges, telco$TotalCharges, xfactors))                        
```

We then attach the continuous variables for that and run the model. $alpha = 1$ means that we are running LASSO. $nlambda = 200$ means we are selecting 200 grid points for $\lambda$.We will decrease $\lambda$ slowly until changes are small or reach 200.

```{r}
lasso_telco<-glmnet(x, y=as.factor(telco$Churn), alpha = 1, family = "binomial", nlambda = 100)   
```

LASSO gives us a **path** of possible models.

```{r}
par(mai=c(.9,.8,.8,.8))
par(mfrow=c(1,1))
plot(lasso_telco, xvar="lambda", label = TRUE, )
```

Here are the dimnames to interpret the graph. We can see that 9 is Fiber optic cable, for example, which is one of the first variables with a non-zero coefficient.

```{r}
dimnames(x)[2]
```

Here's the printed sequence of non-zero coefficients, $R^2$ in terms of deviance, and $\lambda$

```{r}
print(lasso_telco)
```

You can also look at this in terms of $R^2$, or deviance explained.

```{r}
plot(lasso_telco, xvar = "dev", label = TRUE)
```

### Choosing $\lambda$

We use K-fold cross-validation to "tune" $\lambda$, in other words, to choose the right penalty weight $\lambda$ that minimizes validation error.

```{r}
lasso_cv <- cv.glmnet(x, y=telco$Churn, family = "binomial", type.measure = "deviance")
plot(lasso_cv)
```

The coefficients associated with the $\lambda$ that minimizes error are:

```{r}
coef(lasso_cv, s = "lambda.min")
```

Same but with **telco**: Here we can apply that model to a holdout data set

```{r}
# use holdout telco data
xfactors<- model.matrix(Churn ~ SeniorCitizen + Partner + Dependents + PhoneService + MultipleLines + InternetService + OnlineSecurity + OnlineBackup + DeviceProtection + TechSupport + StreamingTV + StreamingMovies + Contract + PaperlessBilling + PaymentMethod, data = telco.holdout)
# remove intercept
xfactors<-xfactors[,-1]

# all continuous variables

x<-as.matrix(data.frame(telco.holdout$tenure, telco.holdout$MonthlyCharges, telco.holdout$TotalCharges, xfactors))
```

```{r}
pred <- predict(lasso_cv, newx=x,  s = "lambda.min", type = "response")
churn <- as.numeric(telco.holdout$Churn)-1

head(cbind(churn, pred))
```

```{r, message=FALSE, warning=FALSE}

par(mfrow=c(1,1))
par(mai=c(.9,.8,.2,.2))
plot(roc(churn, pred), print.auc=TRUE, ylim=c(0,1), levels=c("Churn", "Stay"),
     col="black", lwd=1, main="ROC curve", xlab="Specificity: true negative rate",      ylab="Sensitivity: true positive rate", xlim=c(1,0), direction="<")
```
